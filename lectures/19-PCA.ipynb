{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff704263",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# STA 141B Data & Web Technologies for Data Analysis\n",
    "\n",
    "### Lecture 19, 12/6/23, PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319b7903",
   "metadata": {},
   "source": [
    "### Announcements\n",
    "\n",
    "- last homework due tomorrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa91e341",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Final report \n",
    "\n",
    "- revisit the grading rubric given in lecture 7 \n",
    "- submit: a document (e.g., pdf) with your report. Include a file (e.g., .R or .RMD) with your runnable code. \n",
    "- submit _once per group_! \n",
    "- submit _on time_! (late submissions will not be accepted) \n",
    "- clearly state each team members contribution in the report. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d4a460",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Todays topics\n",
    "\n",
    "- Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672ed198",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "\n",
    "We aim to find a method that reduces high-dimensional data to a lower dimension with as little information loss as possible. Consider the problem in regression: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d03de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv <- read.csv('../data/Advertising.csv')\n",
    "adv <- adv[,-1]\n",
    "adv$Budget <- rowSums(adv[,-4])\n",
    "fitted1 <- lm(Sales ~ Budget, data = adv)$fitted\n",
    "fitted2 <- loess(Sales ~ Budget, data = adv)$fitted\n",
    "\n",
    "    options(repr.plot.width = 20, repr.plot.height = 12)\n",
    "\n",
    "plt1 <- ggplot(adv, aes(Budget, Sales)) + geom_point() + theme_classic()\n",
    "plt2 <- ggplot(adv, aes(Budget, Sales)) + theme_classic() + \n",
    "        geom_point(alpha = 0.1) + \n",
    "        geom_smooth(method = 'lm', se = F) + \n",
    "        geom_point(aes(y=fitted1), color = 'blue') + \n",
    "        geom_segment(aes(xend = Budget, yend = fitted1), color = 'red', alpha = 0.5)\n",
    "plt3 <- ggplot(adv, aes(Budget, Sales)) + theme_classic() + \n",
    "        geom_point(alpha = 0.1) + \n",
    "        geom_smooth(method = 'loess', se = F) + \n",
    "        geom_point(aes(y=fitted2), color = 'blue') + \n",
    "        geom_segment(aes(xend = Budget, yend = fitted2), color = 'red', alpha = 0.5)\n",
    "\n",
    "require(\"ggplot2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0780d1e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "gridExtra::grid.arrange(plt1, plt2, plt3,ncol = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2357bdd0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Principal Component Analysis \n",
    "\n",
    "Consider a data set $X\\in\\mathbb{R}^{n\\times p}$ for $n, p\\in\\mathbb{N}$. \n",
    "Each observation corresponds to a random vector $x = (x_1, \\dots , x_p)'$  with known expectation $E(x) = \\mu\\in\\mathbb{R}^p$ and covariance $Cov(x) = \\Sigma = \\mathbb{R}^{p\\times p}$. \n",
    "\n",
    "Its principal components are linear combinations of $x_1, \\dots , x_p$. Specifically, we aim to find a tranformation of $x$ so that the covarince of $U(x-\\mu)$, $U\\in\\mathbb{R}^{p\\times k}$, $k\\leq p$ has a simple structure and retains as much information about $\\Sigma$ as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaabd2a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let $u_1\\in\\mathbb{R}^p$ so that $Cov(u_1'(x-\\mu)) =  Cov(u_1'x) = u_1'\\Sigma u_1$ is maximal. \n",
    "Since $u_1$ can be arbitrarily large, we limit $\\|u_1\\|^2 = 1$. \n",
    "\n",
    "To maximize $u_1'\\Sigma u_1$ under this constraint, one can use Lagrange multiplication for $\\lambda>1$.  \n",
    "$$\n",
    "\\max u_1'\\Sigma u_1 - \\lambda(\\|u_1\\|^2 - 1). \n",
    "$$\n",
    "Taking derivatives gives $(\\Sigma - \\lambda I_p) u_1 = 0$. Hence, $u_1$ is an eigenvector to eigenvalue $\\lambda$ of $\\Sigma$. \n",
    "Since $u_1'\\Sigma u_1$ ought to be maximized, $\\lambda$ should be as large as possible. \n",
    "Consequently, $u_1$ is an eigenvector to the largest eigenvalue $\\lambda$ of $\\Sigma$. \n",
    "The vector $u_1'(x-\\mu)$ is the first *principal component* of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35b281e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Generally, one can show that the $k$-th principal component is $u_k'(x-\\mu)$ is the eigenvector corresponding to the $k$-th eigenvalue of $\\Sigma$. \n",
    "The vector $u_k\\in\\mathbb{R}^p$ is the vectors of loadings of the $k$-th principal component. \n",
    "\n",
    "For $U\\in\\mathbb{R}^{p\\times k}$ be an orthogonal matrix, which $k$-th column is $u_k$ and $z = U'(x âˆ’ \\mu)\\in\\mathbb{R}^k$, $k \\leq p$ the vector, which $k$-th element is the $k$-th principal component. \n",
    "\n",
    "Note that for $k=p$ we have $$\\Sigma = U\\text{diag}(\\lambda_1, \\dots, \\lambda_p)U'$$ and $$Cov(z) = E(zz') = U'E((x-\\mu)(x-\\mu)')U = U'\\Sigma U = \\text{diag}(\\lambda_1, \\dots, \\lambda_p).$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8099e54",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In practive, $\\mu$ and $\\Sigma$ are unkown. For the data set $X\\in\\mathbb{R}^{n\\times p}$, the empirical principal components are given from the eigenvalue decomposition $\\widehat\\Sigma = \\widehat U\\text{diag}(\\lambda_1, \\dots, \\lambda_p)\\widehat U'$. \n",
    "If $n>p$ one may use the consistent estimators \n",
    "$$\n",
    "\\widehat\\Sigma = \\frac{1}{n - 1}\\widetilde{X}'\\widetilde{X}, \n",
    "$$\n",
    "where $\\widetilde{X}$ is the matrix with entries $\\widetilde{X}_{ij} = x_{ij} - \\bar{x}_j$, $\\bar{x}_j = n^{-1}\\sum_{i=1}^n x_{ij}$ is the estimator for $\\mu_j$. \n",
    "This gives the scores $Z = \\widetilde X \\widehat{U}\\in\\mathbb{R}^{n\\times p}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e83aefe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example: Lets consider the case with $p = 2$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba80e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv <- read.csv('../data/Advertising.csv')\n",
    "n <- nrow(adv)\n",
    "nrow(adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4bc378",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "adv <- adv[,-1]\n",
    "adv$Budget <- rowSums(adv[,-4])\n",
    "head(adv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16583810",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For graphical issues only, lets standardsize the columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dad5ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize <- function(x) (x - min(x)) / (max(x) - min(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46597609",
   "metadata": {},
   "outputs": [],
   "source": [
    "X <- as.data.frame(apply(adv, 2, normalize))[4:5]\n",
    "head(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a86288f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width = 14, repr.plot.height = 8)\n",
    "require('ggplot2')\n",
    "plt <- ggplot(X, aes(Budget, Sales)) + theme_classic() + \n",
    "    xlim(c(-0.4, 1)) + ylim(c(-0.4, 1)) +\n",
    "    geom_point() + coord_fixed() \n",
    "plt + stat_ellipse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f16060e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We know that the density of $x$ is constant on each given contour line, i.e. \n",
    "$$\n",
    "c^2 = x'\\Sigma^{-1} x. \n",
    "$$\n",
    "After eigenvalue decomposition, \n",
    "$$\n",
    "c^2 = x'U\\text{diag}(\\lambda_1^{-1},\\lambda_2^{-1})U'x = \\frac{z_1^2}{\\lambda_1} + \\frac{z_2^2}{\\lambda_2}. \n",
    "$$\n",
    "This equation defines an ellipsoid in the coordinate system $(z_1, z_2)$. As $\\lambda_1>\\lambda_2$, the main axis of this ellipsoid lies in direction of $z_1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516aa09c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "xbar <- colMeans(X)\n",
    "Xtilde <- t(t(X) - xbar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d1a40a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "colMeans(Xtilde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f773c3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "Sigmahat <- t(Xtilde) %*% Xtilde / (n - 1)\n",
    "EIG <- eigen(Sigmahat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d99d949",
   "metadata": {},
   "outputs": [],
   "source": [
    "EIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8041ebb4",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "lambda <- EIG$values\n",
    "Uhat <- EIG$vectors\n",
    "Z <- Xtilde %*% Uhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1348e060",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall: Eigenvectors are orthogonal to each other. Here, they are also normalized (orthonormal). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe8a9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t(Uhat) %*% Uhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f16b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "head(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992f4d22",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Consider the first principal component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb6f058",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope <- Uhat[2, 1] / Uhat[1, 1]\n",
    "intercept <- 0\n",
    " \n",
    "plt + geom_abline(intercept = intercept, slope = slope, \n",
    "                  color = 'blue', linewidth = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54c5b1f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x1 <- (X[, 2] - intercept) / slope\n",
    "y1 <- intercept + slope * X[, 1]\n",
    "x2 <- (x1 + X[, 1]) / 2\n",
    "y2 <- (y1 + X[, 2]) / 2\n",
    "\n",
    "df <- data.frame(X,x2,y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075ede7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt + geom_segment(aes(xend=x2,yend=y2),color=\"red\", alpha = 0.5)+\n",
    "      geom_abline(intercept=0,slope=slope, color = 'blue', linewidth = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c01989",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Second principal component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a2d459",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope <- Uhat[2, 2] / Uhat[1, 2]\n",
    "intercept <- 0 \n",
    "\n",
    "x1 <- (X[, 1] - intercept) / slope\n",
    "y1 <- intercept + slope * X[, 2]\n",
    "x2 <- (x1 + X[, 2]) / 2\n",
    "y2 <- (y1 + X[, 1]) / 2\n",
    "\n",
    "df <- data.frame(X,x2,y2)\n",
    "plt + geom_segment(aes(xend=x2,yend=y2),color=\"red\", alpha = 0.5)+\n",
    "      geom_abline(intercept=0,slope=slope, color = 'blue', linewidth = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19420ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda / sum(lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101e5fff",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The data under the new coordinate system: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ed4563",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ggplot(as.data.frame(Z)) + theme_classic() + geom_point(aes(V1, V2)) + labs(x = \"PC1\", y = \"-PC2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6d63cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Choice of $k$\n",
    "\n",
    "We want to use PCA as a dimension reduction. The $p$-dimensinal data is to be reduced to some $k$-dimensional data, $k\\le p$, while preserving as much variation (information) as possible. Since\n",
    "\n",
    "$$\n",
    "Cov(z) = \\text{diag}(\\lambda_1, \\dots, \\lambda_p), \n",
    "$$\n",
    "\n",
    "this corresponds on choosing the first $k$ empirical components. But how to choose $k$? \n",
    "\n",
    "Consider the [Beans data set](https://archive.ics.uci.edu/ml/datasets/Dry+Bean+Dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130b3b8b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data <- read.table('../data/Dry_Bean_Dataset.csv', sep = ',', header = T)\n",
    "data <- na.omit(data)[,-1]\n",
    "n <- nrow(data)\n",
    "dim(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c04833",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "head(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea09841e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X <- data\n",
    "xbar <- colMeans(X)\n",
    "Xtilde <- t(t(X) - xbar)\n",
    "Sigmahat <- t(Xtilde) %*% Xtilde / (n - 1)\n",
    "EIG <- eigen(Sigmahat)\n",
    "lambda <- EIG$values\n",
    "Uhat <- EIG$vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e12cc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda / sum(lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e366bdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues <- data.frame(index = 1:length(lambda), \n",
    "                          relative_variability = lambda / sum(lambda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c790fd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "require(\"ggplot2\")\n",
    "ggplot(eigenvalues) + theme_minimal() + \n",
    "    labs(x = 'Eigenvalue', y = 'Relative Variability') + \n",
    "    geom_line(aes(index, cumsum(relative_variability))) + \n",
    "    geom_point(aes(index, cumsum(relative_variability))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8ef9e2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The ellbow plot shows the relative variance - the number of PC to be chosen corresponds to the location in the plot, in which the additional variance explained per PC decreases. This is a subjective measure.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9d5fc4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Computation of Eigenvalues\n",
    "\n",
    "The computation of eigenvalues is complicated if $p$ is large. Here, we will learn about an approximate way to compute the $k$ largest eigenvectors and -values.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7699e445",
   "metadata": {},
   "source": [
    "Let $A\\in\\mathbb{R}^{p\\times p}$ be a symmetric positiv semi-definite matrix, so that $A=V\\mbox{diag}(\\mu_1,\\ldots,\\mu_p)V^t$. \n",
    "The columns of $V=(v_1,\\ldots,v_p)$ are the eigenvectors $v_i$ and $\\mu_1>\\mu_2>\\ldots>\\mu_p\\geq 0$. To estimate $v_1$, start with any random vector $v^{(0)}$ and iterate \n",
    "$$\n",
    "v^{(j+1)}_1=\\frac{A^jv^{(0)}}{\\|A^jv^{(0)}\\|},\\;\\;j=1,2,\\ldots\n",
    "$$\n",
    "until convergence. The corresponding eigenvalue is due to the Rayleigh ratio\n",
    "$$\n",
    "\\mu_1=\\frac{v_1^tAv_1}{v_1^tv_1},\n",
    "$$\n",
    "since \n",
    "$$\n",
    "\\frac{v_1^tAv_1}{v_1^tv_1}=\\frac{(Av_1)^tv_1}{v_1^tv_1}=\\frac{\\mu_1 v_1^tv_1}{v_1^tv_1}=\\mu_1.\n",
    "$$\n",
    "The second eigenvector can be retrieved via $A-\\mu_1v_1v_1^t$ (there are other methods as well)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baf7913",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "v^{(j+1)}_1=\\frac{A^jv^{(0)}}{\\|A^jv^{(0)}\\|},\\;\\;j=1,2,\\ldots\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ec8ded",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "A <- Sigmahat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d381ed",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "approx_evec <- function(A) {\n",
    "    v <- runif(nrow(A))\n",
    "    for (i in 1:10) {\n",
    "        v <- A %*% v\n",
    "        v <- v / sqrt(sum(v^2))\n",
    "    }\n",
    "    return (drop(c(v)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420dbcc4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "a <- approx_evec(A)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b26cc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "Uhat[,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e401c406",
   "metadata": {},
   "source": [
    "Note that the eigenvector is identical up to sign. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae59da0d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sum((Uhat[,1] - (- a))^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db666cce",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The corresponding eigenvalue is given: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fd605e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mu1 <- c(t(a) %*% A %*% a / t(a) %*% a)\n",
    "mu1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336cb095",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc3c770",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The second eigenvector is given by: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cde758a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "b <- approx_evec(A - mu1 * a %*% t(a))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b23a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Uhat[,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030ad91d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "v^{(k+1)}_1=\\frac{A^kv^{(0)}}{\\|A^kv^{(0)}\\|},\\;\\;k=1,2,\\ldots\n",
    "$$\n",
    "\n",
    "Why does this work? Since the eigenvectors are orthogonal, each vector $c\\in\\mathbb{R}^p$ can be represented as a linear combination of eigenvectors, i.e., $c=\\sum_{i=1}^p c_iv_i$, $c_i\\geq 0$ with $c_1>0$. Multiplicatoin on both sides with $A$ gives \n",
    "$$\n",
    "Ac=\\sum_{i=1}^pc_iAv_i=\\sum_{i=1}^pc_i\\mu_iv_i.\n",
    "$$\n",
    "$k$-times repetition gives \n",
    "$$\n",
    "A^kc=\\sum_{i=1}^pc_i\\mu_i^kv_i=\\mu_1^k\\left\\{c_1v_1+c_2v_2\\left(\\frac{\\mu_2}{\\mu_1}\\right)^k+\\ldots+c_pv_p\\left(\\frac{\\mu_p}{\\mu_1}\\right)^k\\right\\}.\n",
    "$$\n",
    "For $\\mu_1\\gg\\mu_2$, $A^kc$ converges to $v_1(c_1\\mu_1^k)$ for growing $k$. This convergence can be very slow if $\\mu_1\\approx\\mu_2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ff89a2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How to interpret PC? Suppose `Newspaper` and `Radio` are collinear. Then, we can isolate their effect via a PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a819c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "head(adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fd9f9d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "GGally::ggpairs(adv, columns = 1:3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ec43fd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "summary(lm(Sales~. - Budget, adv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c97121",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X <- adv[,2:4]\n",
    "xbar <- colMeans(X)\n",
    "Xtilde <- t(t(X) - xbar)\n",
    "Sigmahat <- t(Xtilde) %*% Xtilde / (n - 1)\n",
    "EIG <- eigen(Sigmahat)\n",
    "lambda <- EIG$values\n",
    "Uhat <- EIG$vectors\n",
    "Z <- Xtilde %*% Uhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871d9cd6",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "head(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf94af9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "summary(lm(Sales~TV + Z[,1]+ Z[,2], adv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeeb55a",
   "metadata": {},
   "source": [
    "The first PC emulates the effect of `Newspaper` and `Radio`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d78e3e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Downsides of PCA: \n",
    " - If more than one PC are included, their individual effect cannot be related to a real life variable, and hence its more difficult to interpret it\n",
    " - PCA can not reduce the dimensionality of observations on a low-dimensinal non-linear manifold"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "b2b12ec50b0a525a62abe739e766b0c808eccd181e3f804cedbbca00f4d5b392"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
